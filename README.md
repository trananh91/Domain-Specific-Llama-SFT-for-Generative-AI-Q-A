# Domain-Specific Llama SFT for Generative AI Q&A (Updating...)

## About this Project
This project, **Domain-Specific Llama SFT for Generative AI Q&A** was developed following my participation in the **AWS ASEAN LLM League 2025**, an enriching experience hosted by **AI Singapore** and AWS for the ASEAN regions.

The core objective of this initiative is to demonstrate the effectiveness of Supervised Fine-Tuning (SFT) on a Llama model to create a highly specialized generative AI Q&A system for a specific domain.

The AWS ASEAN LLM League competition centers around fine-tuning models with AWS services. For my project, however, I'm exploring and utilizing Nebius AI for this purpose.

---

## Project Structure & Workflow
This project is organized into distinct phases, each with its dedicated documentation:

* **Initial Research:** Understanding the problem space and laying the groundwork.
* **Data Preparation:** Curating and processing the domain-specific dataset.
* **Model Fine-Tuning:** The core process of adapting the Llama model.
* **Evaluation:** Assessing the performance of the fine-tuned model.

---

## Detailed Documented Sections
For in-depth information on each phase, please refer to the dedicated documentation files:

* **Initial Research:** [Read more](./research/initial_research.md)
* **Data Preparation:** [Read more](./data_prep/data_preparation.md)
* **Model Fine-Tuning:** [Read more](./model_ft/model_fine_tuning.md)
* **Evaluation:** [Read more](./eval/evaluation.md)

## Future work
- Exploring different models for optimal performance.
- Deployment of the fine-tuned model as an API service.

## Acknowledgments
- **AWS ASEAN LLM League 2025**: For providing the inspiration and framework for this project.
- **AI Singapore** & **AWS**: Hosts of the LLM League.
- **Nebius AI**: For providing the platform for efficient model fine-tuning.