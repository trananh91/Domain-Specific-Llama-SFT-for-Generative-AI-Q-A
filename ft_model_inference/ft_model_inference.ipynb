{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers peft accelerate bitsandbytes huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload and Unzip the LoRA Adapter\n",
    "After fine-tuning the base model, there wil be two primary files created which are used for Model Inference: `adapter_model.safetensors` and `adapter_config.json`.\n",
    "\n",
    "Steps for the inferencing preparation:\n",
    "1. **Prepare a ZIP file** named `lora_adapter.zip` that contains the following **two required files**:\n",
    "    - `adapter_model.safetensors` â€“ LoRA fine-tuned weights\n",
    "    - `adapter_config.json` â€“ Configuration file describing the adapter\n",
    "\n",
    "2. **Upload the ZIP file** using the file upload tool in the left sidebar or run the `files.upload()` cell below.\n",
    "\n",
    "3. The ZIP will be automatically extracted to `/content/lora_adapter/` for loading with PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can upload via the left-hand \"Files\" tab or use this:\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # upload zip or individual files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If zipped then make a new directory and unzip:\n",
    "!mkdir -p /content/lora_adapter\n",
    "!unzip -o \"*.zip\" -d /content/lora_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Base Model and LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face Hub for model access\n",
    "from huggingface_hub import login\n",
    "\n",
    "HF_TOKEN = \"your_hf_token_here\"\n",
    "login(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "lora_path = \"/content/lora_adapter\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, token=HF_TOKEN)\n",
    "\n",
    "# Load base model in 4-bit quantized mode\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, lora_path)\n",
    "\n",
    "# === ðŸ’¬ STEP 5: Inference ===\n",
    "prompt = \"What is AgenticAI?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge and save the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA weights into base model\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Define save path\n",
    "save_path = \"/content/llama3-8b-merged\"\n",
    "\n",
    "# Save merged model in safetensors format\n",
    "merged_model.save_pretrained(save_path, safe_serialization=True)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# Optional: Save generation config\n",
    "merged_model.generation_config.save_pretrained(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
