{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers peft accelerate bitsandbytes huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload and Unzip the LoRA Adapter\n",
    "After fine-tuning the base model, there wil be two primary files created which are used for Model Inference: `adapter_model.safetensors` and `adapter_config.json`.\n",
    "\n",
    "Steps for the inferencing preparation:\n",
    "1. **Prepare a ZIP file** named `lora_adapter.zip` that contains the following files:\n",
    "    - `adapter_model.safetensors` – LoRA fine-tuned weights\n",
    "    - `adapter_config.json` – Configuration file describing the adapter\n",
    "    - `chat_template.jinja`\n",
    "    - `checkpoint.meta`\n",
    "    - `special_tokens_map.json`\n",
    "    - `tokenizer.json`\n",
    "    - `tokenizer_config.json`\n",
    "\n",
    "2. Since I am using Google Colab to run this Notebook, please **upload the ZIP file** using the file upload tool in the left sidebar or run the `files.upload()` cell below.\n",
    "\n",
    "3. The ZIP will be automatically extracted to `/content/lora_adapter/` for loading with PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can upload via the left-hand \"Files\" tab or use this:\n",
    "from google.colab import files\n",
    "uploaded = files.upload()  # upload zip or individual files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If zipped then make a new directory and unzip:\n",
    "!mkdir -p /content/lora_adapter\n",
    "!unzip -o \"*.zip\" -d /content/lora_adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Base Model and LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since I use Google Colab, I need to set the Hugging Face token as a secret. \n",
    "# Make sure you request the access for the Meta's Llama model repo on Hugging Face and set a new token.\n",
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "# Retrieve your token from Colab Secrets\n",
    "hf_token = userdata.get('hf_inference') # Make sure to set this secret in your Colab environment, replace 'hf_inference' with the key you set\n",
    "\n",
    "# Set it as an environment variable, which Hugging Face libraries will automatically pick up\n",
    "os.environ[\"HF_TOKEN\"] = hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "base_model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "lora_path = \"/content/lora_adapter\" # Check this path matches your uploaded LoRA adapter directory\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_path)\n",
    "print(\"Tokenizer loaded successfully.\")\n",
    "\n",
    "# Load base model in 4-bit quantized mode\n",
    "print(\"Loading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True \n",
    ")\n",
    "print(\"Base model loaded. Loading LoRA adapters...\")\n",
    "\n",
    "# Load the LoRA adapters using Peft's from_pretrained method\n",
    "model = PeftModel.from_pretrained(model, lora_path)\n",
    "print(\"LoRA adapters loaded successfully.\")\n",
    "\n",
    "# --- Merge LoRA Adapters and Base Model (Optional but Recommended for Inference) ---\n",
    "# Merging the LoRA weights into the base model can sometimes improve inference speed\n",
    "# and simplifies deployment by creating a single consolidated model.\n",
    "print(\"Merging LoRA adapters into the base model...\")\n",
    "model = model.merge_and_unload()\n",
    "print(\"LoRA adapters merged.\")\n",
    "\n",
    "# --- Create a Text Generation Pipeline ---\n",
    "# Use Hugging Face's pipeline for easy text generation.\n",
    "print(\"Creating text generation pipeline...\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=200,  # Maximum number of tokens to generate\n",
    "    do_sample=True,      # Enable sampling for more varied outputs\n",
    "    temperature=0.7,     # Controls randomness of predictions\n",
    "    top_p=0.9,           # Nucleus sampling: sample from top p probability tokens\n",
    "    repetition_penalty=1.1 # Penalize repetition\n",
    ")\n",
    "print(\"Pipeline created successfully.\")\n",
    "\n",
    "# --- Perform Inference ---\n",
    "print(\"\\n--- Performing Inference ---\")\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"What is Agentic AI?\"\n",
    "\n",
    "# Apply the chat template if your model expects it (Llama-3.1-Instruct does)\n",
    "# The `chat_template.jinja` file defines how the prompt should be formatted.\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(f\"\\nInput Prompt:\\n{formatted_prompt}\\n\")\n",
    "\n",
    "# Generate text\n",
    "outputs = pipe(formatted_prompt)\n",
    "\n",
    "# Extract and print the generated text\n",
    "generated_text = outputs[0]['generated_text']\n",
    "\n",
    "# Remove the input prompt from the generated text to show only the model's response\n",
    "# This assumes the model echoes the prompt. Adjust if your model behaves differently.\n",
    "response = generated_text[len(formatted_prompt):].strip()\n",
    "\n",
    "print(f\"Generated Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge and Save the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define save path\n",
    "save_path = \"/content/fine_tuned_llama3-8b-merged\"\n",
    "\n",
    "# Save merged model in safetensors format\n",
    "model.save_pretrained(save_path, safe_serialization=True)\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "# Optional: Save generation config\n",
    "model.generation_config.save_pretrained(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
