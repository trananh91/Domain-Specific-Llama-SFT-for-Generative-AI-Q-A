{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Validation and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"raw_data.jsonl\"  # Replace with your raw JSONL file\n",
    "output_file = \"formatted_data.jsonl\"  # Output file for transformed data\n",
    "\n",
    "# Function to validate and transform data\n",
    "def validate_and_transform(input_path, output_path):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as infile, open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line_number, line in enumerate(infile, start=1):\n",
    "            try:\n",
    "                # Parse the JSON line\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # Validate required fields\n",
    "                question = data.get(\"question\") or data.get(\"prompt\")\n",
    "                answer = data.get(\"answer\") or data.get(\"completion\")\n",
    "\n",
    "                if not question or not answer:\n",
    "                    raise ValueError(f\"Missing 'question' or 'answer' fields on line {line_number}\")\n",
    "\n",
    "                # Transform into the expected format\n",
    "                transformed_data = {\n",
    "                    \"prompt\": question.strip(),\n",
    "                    \"completion\": answer.strip()\n",
    "                }\n",
    "\n",
    "                # Write the transformed data to the output file\n",
    "                outfile.write(json.dumps(transformed_data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Invalid JSON format on line {line_number}. Skipping...\")\n",
    "            except ValueError as e:\n",
    "                print(f\"Validation error on line {line_number}: {e}. Skipping...\")\n",
    "\n",
    "    print(f\"Format validation and transformation complete. Output saved to '{output_path}'.\")\n",
    "\n",
    "# Run the validation and transformation\n",
    "validate_and_transform(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Token and Artifact Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"formatted_data.jsonl\"  # Replace with your input JSONL file\n",
    "output_file = \"cleaned_data.jsonl\"  # Output file for cleaned data\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Remove special tokens like [CLS], [SEP], etc.\n",
    "    text = re.sub(r\"\\[CLS\\]|\\[SEP\\]\", \"\", text)\n",
    "\n",
    "    # Remove HTML tags like <a>, <div>, etc.\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "    # Remove control characters like \\n, \\t, etc.\n",
    "    text = re.sub(r\"[\\n\\t\\r]\", \" \", text)\n",
    "\n",
    "    # Remove excessive punctuation (e.g., \"!!!\", \"...\", etc.)\n",
    "    text = re.sub(r\"[!?.]{2,}\", \".\", text)\n",
    "\n",
    "    # Remove non-standard symbols (e.g., Greek alphabets, mathematical formulas)\n",
    "    text = re.sub(r\"[^\\w\\s.,!?'-]\", \"\", text)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    # Trim leading and trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Function to clean and transform JSONL data\n",
    "def clean_and_transform(input_path, output_path):\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as infile, open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for line_number, line in enumerate(infile, start=1):\n",
    "            try:\n",
    "                # Parse the JSON line\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # Clean the \"prompt\" and \"completion\" fields\n",
    "                data[\"prompt\"] = clean_text(data.get(\"prompt\", \"\"))\n",
    "                data[\"completion\"] = clean_text(data.get(\"completion\", \"\"))\n",
    "\n",
    "                # Write the cleaned data to the output file\n",
    "                outfile.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Invalid JSON format on line {line_number}. Skipping...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error on line {line_number}: {e}. Skipping...\")\n",
    "\n",
    "    print(f\"Special token and artifact removal complete. Output saved to '{output_path}'.\")\n",
    "\n",
    "# Run the cleaning and transformation\n",
    "clean_and_transform(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since I use Google Colab, I need to set the Hugging Face token as a secret. \n",
    "# Make sure you request the access for the Meta's Llama model repo on Hugging Face and set a new token.\n",
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "# Retrieve your token from Colab Secrets\n",
    "hf_token = userdata.get('hf_token') # Make sure to set this secret in your Colab environment, replace 'hf_token' with the key you set\n",
    "\n",
    "# Set it as an environment variable, which Hugging Face libraries will automatically pick up\n",
    "os.environ[\"HF_TOKEN\"] = hf_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Hugging Face model ID\n",
    "base_model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Load the tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "print(\"Tokenizer loaded successfully.\")\n",
    "\n",
    "# Path to your JSONL file\n",
    "jsonl_file_path = \"cleaned_data.jsonl\"  # Replace with your file path\n",
    "\n",
    "# Store token counts\n",
    "sample_token_counts = []\n",
    "sample_lines_data = []  # Store full dataset lines\n",
    "\n",
    "# Process JSONL file line by line\n",
    "with open(jsonl_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)  # Parse JSON line\n",
    "        instruction = data.get(\"prompt\", \"\")  # Get instruction text\n",
    "        response = data.get(\"completion\", \"\")  # Get response text\n",
    "\n",
    "        # Compute total token count (instruction + response)\n",
    "        total_tokens = len(tokenizer.encode(instruction, add_special_tokens=True)) + \\\n",
    "                       len(tokenizer.encode(response, add_special_tokens=True))\n",
    "\n",
    "        sample_token_counts.append(total_tokens)\n",
    "        sample_lines_data.append(data)  # Store full JSON object\n",
    "\n",
    "# Convert token count list to NumPy array\n",
    "sample_token_counts = np.array(sample_token_counts)\n",
    "\n",
    "# Compute statistical insights\n",
    "max_tokens = np.max(sample_token_counts)\n",
    "max_index = np.argmax(sample_token_counts)  # Index of the max value\n",
    "max_token_entry = sample_lines_data[max_index]  # Retrieve the corresponding JSON object\n",
    "\n",
    "min_tokens = np.min(sample_token_counts)\n",
    "min_index = np.argmin(sample_token_counts)  # Index of the min value\n",
    "min_token_entry = sample_lines_data[min_index]  # Retrieve the corresponding JSON object\n",
    "\n",
    "mean_tokens = np.mean(sample_token_counts)\n",
    "median_tokens = np.median(sample_token_counts)\n",
    "std_dev = np.std(sample_token_counts)\n",
    "p90 = np.percentile(sample_token_counts, 90)\n",
    "p95 = np.percentile(sample_token_counts, 95)\n",
    "\n",
    "# Print statistics\n",
    "print(f\"ðŸ“Š Token Count Statistics:\")\n",
    "print(f\"- Min tokens: {min_tokens}\")\n",
    "print(f\"- Max tokens: {max_tokens}\")\n",
    "print(f\"- Mean tokens: {mean_tokens:.2f}\")\n",
    "print(f\"- Median tokens: {median_tokens}\")\n",
    "print(f\"- Standard deviation: {std_dev:.2f}\")\n",
    "print(f\"- 90th percentile: {p90}\")\n",
    "print(f\"- 95th percentile: {p95}\")\n",
    "\n",
    "# Print details of max and min token entries\n",
    "print(f\"ðŸ“Œ Line with the most tokens ({max_tokens} tokens):\")\n",
    "print(json.dumps(max_token_entry, indent=4, ensure_ascii=False))\n",
    "\n",
    "print(f\"ðŸ“Œ Line with the fewest tokens ({min_tokens} tokens):\")\n",
    "print(json.dumps(min_token_entry, indent=4, ensure_ascii=False))\n",
    "\n",
    "# --- Visualization ---\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# ðŸ”¹ Histogram Plot (Token Count Distribution)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(sample_token_counts, bins=30, kde=True, color=\"blue\")\n",
    "plt.axvline(p90, color='r', linestyle='dashed', label=f'90th Percentile: {p90:.0f}')\n",
    "plt.axvline(p95, color='orange', linestyle='dashed', label=f'95th Percentile: {p95:.0f}')\n",
    "plt.xlabel(\"Token Count (instruction + response)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Token Counts\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ðŸ”¹ Boxplot (Detect Outliers)\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.boxplot(x=sample_token_counts, color=\"purple\")\n",
    "plt.title(\"Boxplot of Token Counts\")\n",
    "plt.xlabel(\"Token Count (instruction + response)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Curation and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = '''\n",
    "You are an expert in Generative AI with deep knowledge of the domains of four subtopic: Foundation Models, Responsible AI, Agentic AI and Prompt Engineering.\n",
    "You will be given a question related to one of these subtopics and an answer to that question. They will be used to instruction fine tune a large language model on Generative AI.\n",
    "Your tasks are:\n",
    "1. Fact-check the given answer: Verify the accuracy and comprehensiveness of the provided answer. If you find any inaccuracies, provide the best possible, verified answer to the question.\n",
    "2. Refine the answer: If the answer is factually correct, your task is to refine it to make it more accurate and comprehensive.\n",
    "\n",
    "Your answer will be evaluated based on two primary criteria: Accuracy and Comprehensiveness. You MUST ensure your answer have:\n",
    "- Maximum accuracy: Factually accurate, well-researched and precise.\n",
    "- High comprehensiveness: Every answer should be reasonably long and detailed. But MUST not too long.\n",
    "- Target length: MUST be 350 to 400 words.\n",
    "- Diverse viewpoints and key points: Include multiple key aspects and perspectives with the concise discussions on each.\n",
    "- Provide ONE most relevant example if possible to support the answer.\n",
    "- Well-structured format: Use Markdown format for clear formatting and better readability.\n",
    "- No hallucination: Stick to verifiable facts.\n",
    "\n",
    "For every answer, you MUST reponse in Markdown format. For example, your answer can follow this format which has some Markdown elements. But you do not need to strictly include all the following elements, just choose which are necessary:\n",
    "\"\n",
    "## The Introduction or Overview of the answer (MAXIMUM 1 sentence): A straightforward introduction. \\n\\n\n",
    "#### Key points for the answer (MAXIMUM 2 key points): \\n\\n\n",
    "- **Key point 1**: Discussion on key point 1. \\n \n",
    "- **Key point 2**: Discussion on key point 2. \\n \n",
    "#### Different viewpoints or perspectives (MAXIMUM 2 viewpoints): \\n\\n\n",
    "- ** Viewpoint 1**: Analysis on viewpoint 1. \\n\n",
    "- ** Viewpoint 2**: Analysis on viewpoint 2. \\n\n",
    "#### Example: (MAXIMUM 1 example): Most relevant example to support the answer.\\n\\n\n",
    "## The final conclusion for the answer (MAXIMUM 1 sentence): A concise conclusion.\\n\\n\n",
    "\"\n",
    "\n",
    "Important Guidelines for each answer: \n",
    "- Include Headings, Titles, Numbers, Bullets, Capital Letters, and Bold or Italics to improve organization and make the answer easier to navigate.\n",
    "- MUST NOT include special characters like Greek alphabets, mathematical formulas, or any other non-standard symbols.\n",
    "- When listing items or using bullet list, list MAXIMUM 2 items.\n",
    "- When doing comparisons, compare on MAXIMUM 2 aspects. Examples are not needed for comparisons.\n",
    "- For answers explaining many implementation stages, just include the names of the stages. MUST NOT break down into details of each stage.\n",
    "- Introduction and conclusion is a MUST and should be included in every answer. \n",
    "\n",
    "Output only your answer without any additional text or explanation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "from IPython.display import Markdown\n",
    "\n",
    "class GPT:\n",
    "    def __init__(self, system_prompt=SYSTEM_PROMPT, api_key=API_KEY, model='gpt-4o'):\n",
    "        self.client = openai.OpenAI(api_key=api_key)\n",
    "        self.model = model\n",
    "        self.system_prompt = system_prompt\n",
    "    \n",
    "    def refine_answer(self, prompt, completion):       \n",
    "        try: \n",
    "            user_prompt = f\"Here is the given question:\\n{prompt}\\n\\nHere is the given answer:\\n{completion}\"\n",
    "            \n",
    "            completion = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0.4,\n",
    "                top_p=0.8\n",
    "            )\n",
    "            return completion.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error refining response: {e}\")\n",
    "            return None\n",
    "\n",
    "# Read the JSONL file, refine responses, and save to a new JSONL file\n",
    "def process_jsonl(input_file, output_file):\n",
    "    gpt = GPT()\n",
    "    \n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as infile, open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        for index, line in enumerate(infile):\n",
    "            try:\n",
    "                # Parse the JSON line\n",
    "                data = json.loads(line)\n",
    "                instruction = data.get(\"prompt\", \"\")\n",
    "                response = data.get(\"completion\", \"\")\n",
    "                \n",
    "                if response:\n",
    "                    # Refine the response\n",
    "                    refined_response = gpt.refine_answer(instruction, response)\n",
    "                    if refined_response:\n",
    "                        # Create a new JSON object with only instruction and refined response\n",
    "                        new_data = {\"prompt\": instruction, \"completion\": refined_response}\n",
    "                        # Write the new data to the output file\n",
    "                        outfile.write(json.dumps(new_data) + \"\\n\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON line: {e}\")\n",
    "                print(f\"======================= Error Line {index+1}\") \n",
    "\n",
    "# Example usage\n",
    "input_file = r\"cleaned_data.jsonl\"  # Replace with your input JSONL file path\n",
    "output_file = r\"dataset.jsonl\"  # Replace with your desired output JSONL file path\n",
    "process_jsonl(input_file, output_file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
